{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA on 🤗 Transformers 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 로깅 메시지 제거용\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/28896432/80024445-0f444e00-851a-11ea-9137-9da2abfd553d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Discriminator\n",
    "- [electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator#how-to-use-the-discriminator-in-transformers)\n",
    "- Fake Token Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad199e3a5f4be4af4900643b2ca850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858ac8e7845a48568d6fc685f28f22ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443133216.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0dd0d1d45d4e48be9e076c25beab3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=279173.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0324391ef2464f6bbef576d6ba346cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=51.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ElectraForPreTraining, ElectraTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 왜 밥을 먹었다. -[encode]-> tensor([[    2, 10841,  3579, 21509, 27660,    18,     3]])\n",
      "tensor([    2, 10841,  3579, 21509, 27660,    18,     3]) -[decode]-> [CLS] 나는 왜 밥을 먹었다. [SEP]\n",
      "\n",
      "나는 왜 밥을 먹었다. -[tokenize]-> ['나는', '왜', '밥을', '먹었다', '.']\n",
      "['나는', '왜', '밥을', '먹었다', '.'] -[convert_tokens_to_ids]-> [10841, 3579, 21509, 27660, 18]\n",
      "[10841, 3579, 21509, 27660, 18] -[convert_ids_to_tokens]-> ['나는', '왜', '밥을', '먹었다', '.']\n"
     ]
    }
   ],
   "source": [
    "fake_sentence = \"나는 왜 밥을 먹었다.\"\n",
    "fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\") # tensors\n",
    "?tokenizer.encode\n",
    "print(fake_sentence, '-[encode]->', fake_inputs)\n",
    "print(fake_inputs[0], '-[decode]->', tokenizer.decode(fake_inputs[0])) # a string /w special token\n",
    "print()\n",
    "\n",
    "fake_tokens = tokenizer.tokenize(fake_sentence)\n",
    "fake_ids = tokenizer.convert_tokens_to_ids(fake_tokens) # list of int\n",
    "print(fake_sentence, '-[tokenize]->', fake_tokens)\n",
    "print(fake_tokens, '-[convert_tokens_to_ids]->', fake_ids)\n",
    "print(fake_ids, '-[convert_ids_to_tokens]->', tokenizer.convert_ids_to_tokens(fake_ids))\n",
    "\n",
    "# sentence = \"나는 방금 밥을 먹었다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 10841,  3579, 21509, 27660,    18,     3]])\n",
      "y: tensor([-10.2365,  -2.9496,   1.1244,  -3.4443,  -2.1990,  -3.4055, -10.2365])\n",
      "sin(y): tensor([-1., -1.,  1., -1., -1., -1., -1.])\n",
      "(sin(y)+1)/2: tensor([0., 0., 1., 0., 0., 0., 0.])\n",
      "[('나는', 0.0), ('왜', 1.0), ('밥을', 0.0), ('먹었다', 0.0), ('.', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "discriminator_outputs = discriminator(fake_inputs)\n",
    "print(fake_inputs)\n",
    "print('y:', discriminator_outputs[0].data)\n",
    "print('sin(y):', torch.sign(discriminator_outputs[0]).data)\n",
    "print('(sin(y)+1)/2:', ((torch.sign(discriminator_outputs[0])+1)/2).data)\n",
    "predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
    "\n",
    "pprint(list(zip(fake_tokens, predictions.tolist()[1:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generator\n",
    "\n",
    "- [electra-base-generator](https://huggingface.co/google/electra-base-generator#how-to-use-the-generator-in-transformers)\n",
    "- 기존 BERT의 Mask Token Prediction과 동일하다고 생각하면 됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 [MASK] 밥을 먹었다.\n",
      "[{'score': 0.07130879908800125,\n",
      "  'sequence': '[CLS] 나는 식당에서 밥을 먹었다. [SEP]',\n",
      "  'token': 26194,\n",
      "  'token_str': '식당에서'},\n",
      " {'score': 0.04359052702784538,\n",
      "  'sequence': '[CLS] 나는 방금 밥을 먹었다. [SEP]',\n",
      "  'token': 24499,\n",
      "  'token_str': '방금'},\n",
      " {'score': 0.029709946364164352,\n",
      "  'sequence': '[CLS] 나는 다시 밥을 먹었다. [SEP]',\n",
      "  'token': 10715,\n",
      "  'token_str': '다시'},\n",
      " {'score': 0.02787844091653824,\n",
      "  'sequence': '[CLS] 나는 앉아서 밥을 먹었다. [SEP]',\n",
      "  'token': 23755,\n",
      "  'token_str': '앉아서'},\n",
      " {'score': 0.025679906830191612,\n",
      "  'sequence': '[CLS] 나는 내 밥을 먹었다. [SEP]',\n",
      "  'token': 783,\n",
      "  'token_str': '내'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"monologg/koelectra-base-generator\",\n",
    "    tokenizer=\"monologg/koelectra-base-generator\"\n",
    ")\n",
    "s = \"나는 {} 밥을 먹었다.\".format(fill_mask.tokenizer.mask_token)\n",
    "print(s)\n",
    "pprint(fill_mask(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail Review for Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pretraining의 경우는 원 저자의 Tensorflow 코드를 쓰는 것을 추천\n",
    "- Huggingface Transformers의 코드는 Pretraining이 완료된 모델을 가져다 쓰는 용도\n",
    "- [modeling_electra.py](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.modeling_bert import BertEmbeddings\n",
    "from transformers.modeling_electra import ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ElectraEmbeddings\n",
    "\n",
    "BertEmbeddings와 동일!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraEmbeddings(BertEmbeddings):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ElectraModel\n",
    "\n",
    "BertModel과 동일하지만 Pooler는 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ElectraForPreTraining\n",
    "\n",
    "- Electra model with a binary classification head on top as used during pre-training for identifying generated tokens.\n",
    "- 개인적으로 이 클래스 이름을 좋아하진 않음\n",
    "- 논문에서는 `ElectraModel`도 discriminator의 것을 사용하라고 하고 있음\n",
    "\n",
    "```python\n",
    "model = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining\n",
    "\n",
    "model = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nn.Linear(config.hidden_size, 1)`를 통과한 후, `BCEWithLogitsLoss`를 사용하여 Sigmoid 적용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraDiscriminatorPredictions(nn.Module):\n",
    "    \"\"\"Prediction module for the discriminator, made up of two dense layers.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dense_prediction = nn.Linear(config.hidden_size, 1)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, discriminator_hidden_states, attention_mask):\n",
    "        hidden_states = self.dense(discriminator_hidden_states)\n",
    "        hidden_states = get_activation(self.config.hidden_act)(hidden_states)\n",
    "        logits = self.dense_prediction(hidden_states).squeeze()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraForPreTraining(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.discriminator_predictions = ElectraDiscriminatorPredictions(config)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "        discriminator_hidden_states = self.electra(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "        )\n",
    "        discriminator_sequence_output = discriminator_hidden_states[0]\n",
    "\n",
    "        logits = self.discriminator_predictions(discriminator_sequence_output, attention_mask)\n",
    "\n",
    "        output = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1, discriminator_sequence_output.shape[1]) == 1\n",
    "                active_logits = logits.view(-1, discriminator_sequence_output.shape[1])[active_loss]\n",
    "                active_labels = labels[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels.float())\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, discriminator_sequence_output.shape[1]), labels.float())\n",
    "\n",
    "            output = (loss,) + output\n",
    "\n",
    "        output += discriminator_hidden_states[1:]\n",
    "\n",
    "        return output  # (loss), scores, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ElectraForMaskedLM\n",
    "\n",
    "- Electra model with a language modeling head on top.\n",
    "- 우리가 아는 BERT의 Masked Token Prediction\n",
    "\n",
    "```python\n",
    "model = ElectraForMaskedLM.from_pretrained('google/electra-base-generator')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForMaskedLM\n",
    "\n",
    "model = ElectraForMaskedLM.from_pretrained('google/electra-base-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraGeneratorPredictions(nn.Module):\n",
    "    \"\"\"Prediction module for the generator, made up of two dense layers.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.embedding_size)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "\n",
    "    def forward(self, generator_hidden_states):\n",
    "        hidden_states = self.dense(generator_hidden_states)\n",
    "        hidden_states = get_activation(\"gelu\")(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraForMaskedLM(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.generator_predictions = ElectraGeneratorPredictions(config)\n",
    "\n",
    "        self.generator_lm_head = nn.Linear(config.embedding_size, config.vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.generator_lm_head\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if \"masked_lm_labels\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"masked_lm_labels\")\n",
    "        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n",
    "\n",
    "        generator_hidden_states = self.electra(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "        )\n",
    "        generator_sequence_output = generator_hidden_states[0]\n",
    "\n",
    "        prediction_scores = self.generator_predictions(generator_sequence_output)\n",
    "        prediction_scores = self.generator_lm_head(prediction_scores)\n",
    "\n",
    "        output = (prediction_scores,)\n",
    "\n",
    "        # Masked language modeling softmax layer\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n",
    "            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            output = (loss,) + output\n",
    "\n",
    "        output += generator_hidden_states[1:]\n",
    "\n",
    "        return output  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
